{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c7d199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:15:20.110264Z",
     "iopub.status.busy": "2023-12-17T05:15:20.109903Z",
     "iopub.status.idle": "2023-12-17T05:15:25.278355Z",
     "shell.execute_reply": "2023-12-17T05:15:25.277532Z"
    },
    "papermill": {
     "duration": 5.180351,
     "end_time": "2023-12-17T05:15:25.280776",
     "exception": false,
     "start_time": "2023-12-17T05:15:20.100425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc  \n",
    "import os  \n",
    "import time  \n",
    "import warnings \n",
    "from itertools import combinations  \n",
    "from warnings import simplefilter \n",
    "import joblib  \n",
    "import lightgbm as lgb  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  \n",
    "import polars as pl\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "is_offline = False \n",
    "LGB = True\n",
    "NN = False\n",
    "is_train = True  \n",
    "is_infer = True \n",
    "max_lookback = np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3539fff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:15:25.339307Z",
     "iopub.status.busy": "2023-12-17T05:15:25.339059Z",
     "iopub.status.idle": "2023-12-17T05:15:25.357279Z",
     "shell.execute_reply": "2023-12-17T05:15:25.356183Z"
    },
    "papermill": {
     "duration": 0.029194,
     "end_time": "2023-12-17T05:15:25.359481",
     "exception": false,
     "start_time": "2023-12-17T05:15:25.330287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc86ac39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:15:25.376280Z",
     "iopub.status.busy": "2023-12-17T05:15:25.376036Z",
     "iopub.status.idle": "2023-12-17T05:15:25.387617Z",
     "shell.execute_reply": "2023-12-17T05:15:25.386881Z"
    },
    "papermill": {
     "duration": 0.022053,
     "end_time": "2023-12-17T05:15:25.389589",
     "exception": false,
     "start_time": "2023-12-17T05:15:25.367536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbcbe2",
   "metadata": {
    "papermill": {
     "duration": 0.008522,
     "end_time": "2023-12-17T05:15:25.406439",
     "exception": false,
     "start_time": "2023-12-17T05:15:25.397917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03635f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:15:25.425340Z",
     "iopub.status.busy": "2023-12-17T05:15:25.424999Z",
     "iopub.status.idle": "2023-12-17T05:15:45.249343Z",
     "shell.execute_reply": "2023-12-17T05:15:45.248325Z"
    },
    "papermill": {
     "duration": 19.83648,
     "end_time": "2023-12-17T05:15:45.251736",
     "exception": false,
     "start_time": "2023-12-17T05:15:25.415256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./input/optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_shape = df.shape\n",
    "df['target_shift1'] = df.groupby(['stock_id','seconds_in_bucket'])['target'].shift(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a41a1",
   "metadata": {
    "papermill": {
     "duration": 0.007249,
     "end_time": "2023-12-17T05:15:45.306016",
     "exception": false,
     "start_time": "2023-12-17T05:15:45.298767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Parallel Triplet Imbalance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019ee822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:15:45.322460Z",
     "iopub.status.busy": "2023-12-17T05:15:45.322189Z",
     "iopub.status.idle": "2023-12-17T05:15:45.937839Z",
     "shell.execute_reply": "2023-12-17T05:15:45.937093Z"
    },
    "papermill": {
     "duration": 0.626421,
     "end_time": "2023-12-17T05:15:45.940106",
     "exception": false,
     "start_time": "2023-12-17T05:15:45.313685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af0d28d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:15:45.956811Z",
     "iopub.status.busy": "2023-12-17T05:15:45.956503Z",
     "iopub.status.idle": "2023-12-17T05:15:45.968523Z",
     "shell.execute_reply": "2023-12-17T05:15:45.967803Z"
    },
    "papermill": {
     "duration": 0.0224,
     "end_time": "2023-12-17T05:15:45.970340",
     "exception": false,
     "start_time": "2023-12-17T05:15:45.947940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459da453",
   "metadata": {
    "papermill": {
     "duration": 0.007454,
     "end_time": "2023-12-17T05:15:45.985763",
     "exception": false,
     "start_time": "2023-12-17T05:15:45.978309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Generation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6aacf22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:15:46.002655Z",
     "iopub.status.busy": "2023-12-17T05:15:46.002368Z",
     "iopub.status.idle": "2023-12-17T05:15:46.028229Z",
     "shell.execute_reply": "2023-12-17T05:15:46.027498Z"
    },
    "papermill": {
     "duration": 0.036749,
     "end_time": "2023-12-17T05:15:46.030147",
     "exception": false,
     "start_time": "2023-12-17T05:15:45.993398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "\n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    ss = df.groupby('time_id')['weighted_wap'].sum()/df.groupby('time_id')['stock_weights'].sum()\n",
    "    ss = ss.reset_index()\n",
    "    ss.columns = ['time_id','wapindex']\n",
    "    df = pd.merge(df,ss,how='left',on='time_id')\n",
    "    df['wapdiff'] = df['wap'] - df['wapindex']\n",
    "    \n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "   \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    \n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    \n",
    "    #V4 feature\n",
    "    for window in [3,5,10]:\n",
    "        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n",
    "        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n",
    "\n",
    "    #V5 - rolling diff\n",
    "    # Convert from pandas to Polars\n",
    "    pl_df = pl.from_pandas(df)\n",
    "\n",
    "    #Define the windows and columns for which you want to calculate the rolling statistics\n",
    "    windows = [3, 5, 10]\n",
    "    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n",
    "\n",
    "    # prepare the operations for each column and window\n",
    "    group = [\"stock_id\"]\n",
    "    expressions = []\n",
    "\n",
    "    # Loop over each window and column to create the rolling mean and std expressions\n",
    "    for window in windows:\n",
    "        for col in columns:\n",
    "            rolling_mean_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_mean(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            rolling_std_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_std(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_std_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            expressions.append(rolling_mean_expr)\n",
    "            expressions.append(rolling_std_expr)\n",
    "\n",
    "    # Run the operations using Polars' lazy API\n",
    "    lazy_df = pl_df.lazy().with_columns(expressions)\n",
    "\n",
    "    # Execute the lazy expressions and overwrite the pl_df variable\n",
    "    pl_df = lazy_df.collect()\n",
    "\n",
    "    # Convert back to pandas if necessary\n",
    "    df = pl_df.to_pandas()\n",
    "    gc.collect()\n",
    "    \n",
    "    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n",
    "    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    df['time_to_market_close'] = 540 - df['seconds_in_bucket']\n",
    "    \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "#     cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "#     df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    gc.collect() \n",
    "    df = other_features(df)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028c90f",
   "metadata": {
    "papermill": {
     "duration": 0.007267,
     "end_time": "2023-12-17T05:15:46.045697",
     "exception": false,
     "start_time": "2023-12-17T05:15:46.038430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96cfa2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:15:46.062374Z",
     "iopub.status.busy": "2023-12-17T05:15:46.061719Z",
     "iopub.status.idle": "2023-12-17T05:15:46.067132Z",
     "shell.execute_reply": "2023-12-17T05:15:46.066298Z"
    },
    "papermill": {
     "duration": 0.016149,
     "end_time": "2023-12-17T05:15:46.069294",
     "exception": false,
     "start_time": "2023-12-17T05:15:46.053145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = df\n",
    "print(\"Online mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2059a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:15:46.086020Z",
     "iopub.status.busy": "2023-12-17T05:15:46.085717Z",
     "iopub.status.idle": "2023-12-17T05:17:23.354372Z",
     "shell.execute_reply": "2023-12-17T05:17:23.353405Z"
    },
    "papermill": {
     "duration": 97.279675,
     "end_time": "2023-12-17T05:17:23.356889",
     "exception": false,
     "start_time": "2023-12-17T05:15:46.077214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Online Train Feats Finished.\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c2a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f0848e7",
   "metadata": {
    "papermill": {
     "duration": 0.007705,
     "end_time": "2023-12-17T05:17:23.412303",
     "exception": false,
     "start_time": "2023-12-17T05:17:23.404598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Model Training**  **LGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a0b104d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:17:23.444406Z",
     "iopub.status.busy": "2023-12-17T05:17:23.444111Z",
     "iopub.status.idle": "2023-12-17T08:50:03.566448Z",
     "shell.execute_reply": "2023-12-17T08:50:03.565568Z"
    },
    "papermill": {
     "duration": 12760.133827,
     "end_time": "2023-12-17T08:50:03.569246",
     "exception": false,
     "start_time": "2023-12-17T05:17:23.435419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 163\n",
      "Fold 1 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 7.09107\n",
      "[200]\tvalid_0's l1: 7.04549\n",
      "[300]\tvalid_0's l1: 7.01949\n",
      "[400]\tvalid_0's l1: 6.99824\n",
      "[500]\tvalid_0's l1: 6.98035\n",
      "[600]\tvalid_0's l1: 6.96423\n",
      "[700]\tvalid_0's l1: 6.94995\n",
      "[800]\tvalid_0's l1: 6.93696\n",
      "[900]\tvalid_0's l1: 6.92521\n",
      "[1000]\tvalid_0's l1: 6.9144\n",
      "[1100]\tvalid_0's l1: 6.90367\n",
      "[1200]\tvalid_0's l1: 6.89413\n",
      "[1300]\tvalid_0's l1: 6.88429\n",
      "[1400]\tvalid_0's l1: 6.87551\n",
      "[1500]\tvalid_0's l1: 6.86734\n",
      "[1600]\tvalid_0's l1: 6.86012\n",
      "[1700]\tvalid_0's l1: 6.85253\n",
      "[1800]\tvalid_0's l1: 6.84543\n",
      "[1900]\tvalid_0's l1: 6.83879\n",
      "[2000]\tvalid_0's l1: 6.83129\n",
      "[2100]\tvalid_0's l1: 6.8236\n",
      "[2200]\tvalid_0's l1: 6.81715\n",
      "[2300]\tvalid_0's l1: 6.81044\n",
      "[2400]\tvalid_0's l1: 6.8042\n",
      "[2500]\tvalid_0's l1: 6.79679\n",
      "[2600]\tvalid_0's l1: 6.79074\n",
      "[2700]\tvalid_0's l1: 6.78443\n",
      "[2800]\tvalid_0's l1: 6.77811\n",
      "[2900]\tvalid_0's l1: 6.77198\n",
      "[3000]\tvalid_0's l1: 6.76641\n",
      "[3100]\tvalid_0's l1: 6.76044\n",
      "[3200]\tvalid_0's l1: 6.75435\n",
      "[3300]\tvalid_0's l1: 6.74891\n",
      "[3400]\tvalid_0's l1: 6.74359\n",
      "[3500]\tvalid_0's l1: 6.73854\n",
      "[3600]\tvalid_0's l1: 6.73312\n",
      "[3700]\tvalid_0's l1: 6.72773\n",
      "[3800]\tvalid_0's l1: 6.72287\n",
      "[3900]\tvalid_0's l1: 6.71739\n",
      "[4000]\tvalid_0's l1: 6.71247\n",
      "[4100]\tvalid_0's l1: 6.70798\n",
      "[4200]\tvalid_0's l1: 6.70339\n",
      "[4300]\tvalid_0's l1: 6.69862\n",
      "[4400]\tvalid_0's l1: 6.69363\n",
      "[4500]\tvalid_0's l1: 6.68907\n",
      "[4600]\tvalid_0's l1: 6.68422\n",
      "[4700]\tvalid_0's l1: 6.67873\n",
      "[4800]\tvalid_0's l1: 6.67406\n",
      "[4900]\tvalid_0's l1: 6.66901\n",
      "[5000]\tvalid_0's l1: 6.66392\n",
      "[5100]\tvalid_0's l1: 6.65931\n",
      "[5200]\tvalid_0's l1: 6.65471\n",
      "[5300]\tvalid_0's l1: 6.64999\n",
      "[5400]\tvalid_0's l1: 6.64532\n",
      "[5500]\tvalid_0's l1: 6.64092\n",
      "[5600]\tvalid_0's l1: 6.63625\n",
      "[5700]\tvalid_0's l1: 6.6312\n",
      "[5800]\tvalid_0's l1: 6.62647\n",
      "[5900]\tvalid_0's l1: 6.62216\n",
      "[6000]\tvalid_0's l1: 6.61724\n",
      "[6100]\tvalid_0's l1: 6.61331\n",
      "[6200]\tvalid_0's l1: 6.60901\n",
      "[6300]\tvalid_0's l1: 6.60464\n",
      "[6400]\tvalid_0's l1: 6.60009\n",
      "[6500]\tvalid_0's l1: 6.59588\n",
      "[6600]\tvalid_0's l1: 6.59139\n",
      "[6700]\tvalid_0's l1: 6.58708\n",
      "[6800]\tvalid_0's l1: 6.58249\n",
      "[6900]\tvalid_0's l1: 6.57827\n",
      "[7000]\tvalid_0's l1: 6.57373\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[7000]\tvalid_0's l1: 6.57373\n",
      "Model for fold 1 saved to modelitos_para_despues\\doblez_1.txt\n",
      ":LGB Fold 1 MAE: 6.573733387509769\n",
      "Fold 2 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.41783\n",
      "[200]\tvalid_0's l1: 6.38626\n",
      "[300]\tvalid_0's l1: 6.36515\n",
      "[400]\tvalid_0's l1: 6.34699\n",
      "[500]\tvalid_0's l1: 6.33026\n",
      "[600]\tvalid_0's l1: 6.31579\n",
      "[700]\tvalid_0's l1: 6.30237\n",
      "[800]\tvalid_0's l1: 6.28936\n",
      "[900]\tvalid_0's l1: 6.27754\n",
      "[1000]\tvalid_0's l1: 6.26691\n",
      "[1100]\tvalid_0's l1: 6.25746\n",
      "[1200]\tvalid_0's l1: 6.24976\n",
      "[1300]\tvalid_0's l1: 6.24204\n",
      "[1400]\tvalid_0's l1: 6.23462\n",
      "[1500]\tvalid_0's l1: 6.22732\n",
      "[1600]\tvalid_0's l1: 6.2205\n",
      "[1700]\tvalid_0's l1: 6.21334\n",
      "[1800]\tvalid_0's l1: 6.20584\n",
      "[1900]\tvalid_0's l1: 6.19918\n",
      "[2000]\tvalid_0's l1: 6.19286\n",
      "[2100]\tvalid_0's l1: 6.18596\n",
      "[2200]\tvalid_0's l1: 6.17978\n",
      "[2300]\tvalid_0's l1: 6.17428\n",
      "[2400]\tvalid_0's l1: 6.16825\n",
      "[2500]\tvalid_0's l1: 6.16186\n",
      "[2600]\tvalid_0's l1: 6.15548\n",
      "[2700]\tvalid_0's l1: 6.14911\n",
      "[2800]\tvalid_0's l1: 6.14295\n",
      "[2900]\tvalid_0's l1: 6.13679\n",
      "[3000]\tvalid_0's l1: 6.13118\n",
      "[3100]\tvalid_0's l1: 6.12534\n",
      "[3200]\tvalid_0's l1: 6.11892\n",
      "[3300]\tvalid_0's l1: 6.11297\n",
      "[3400]\tvalid_0's l1: 6.10741\n",
      "[3500]\tvalid_0's l1: 6.10189\n",
      "[3600]\tvalid_0's l1: 6.09586\n",
      "[3700]\tvalid_0's l1: 6.09028\n",
      "[3800]\tvalid_0's l1: 6.08476\n",
      "[3900]\tvalid_0's l1: 6.07971\n",
      "[4000]\tvalid_0's l1: 6.07476\n",
      "[4100]\tvalid_0's l1: 6.06997\n",
      "[4200]\tvalid_0's l1: 6.0651\n",
      "[4300]\tvalid_0's l1: 6.06001\n",
      "[4400]\tvalid_0's l1: 6.05575\n",
      "[4500]\tvalid_0's l1: 6.05082\n",
      "[4600]\tvalid_0's l1: 6.04624\n",
      "[4700]\tvalid_0's l1: 6.04212\n",
      "[4800]\tvalid_0's l1: 6.03799\n",
      "[4900]\tvalid_0's l1: 6.03288\n",
      "[5000]\tvalid_0's l1: 6.02834\n",
      "[5100]\tvalid_0's l1: 6.02405\n",
      "[5200]\tvalid_0's l1: 6.0189\n",
      "[5300]\tvalid_0's l1: 6.0144\n",
      "[5400]\tvalid_0's l1: 6.00953\n",
      "[5500]\tvalid_0's l1: 6.00581\n",
      "[5600]\tvalid_0's l1: 6.00139\n",
      "[5700]\tvalid_0's l1: 5.99707\n",
      "[5800]\tvalid_0's l1: 5.99271\n",
      "[5900]\tvalid_0's l1: 5.98837\n",
      "[6000]\tvalid_0's l1: 5.98453\n",
      "[6100]\tvalid_0's l1: 5.98003\n",
      "[6200]\tvalid_0's l1: 5.97601\n",
      "[6300]\tvalid_0's l1: 5.97164\n",
      "[6400]\tvalid_0's l1: 5.96744\n",
      "[6500]\tvalid_0's l1: 5.96322\n",
      "[6600]\tvalid_0's l1: 5.95894\n",
      "[6700]\tvalid_0's l1: 5.95454\n",
      "[6800]\tvalid_0's l1: 5.95055\n",
      "[6900]\tvalid_0's l1: 5.94692\n",
      "[7000]\tvalid_0's l1: 5.94293\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[7000]\tvalid_0's l1: 5.94293\n",
      "Model for fold 2 saved to modelitos_para_despues\\doblez_2.txt\n",
      ":LGB Fold 2 MAE: 5.942931569528515\n",
      "Fold 3 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.33031\n",
      "[200]\tvalid_0's l1: 6.29487\n",
      "[300]\tvalid_0's l1: 6.26898\n",
      "[400]\tvalid_0's l1: 6.24787\n",
      "[500]\tvalid_0's l1: 6.22963\n",
      "[600]\tvalid_0's l1: 6.21254\n",
      "[700]\tvalid_0's l1: 6.1967\n",
      "[800]\tvalid_0's l1: 6.18186\n",
      "[900]\tvalid_0's l1: 6.16909\n",
      "[1000]\tvalid_0's l1: 6.15715\n",
      "[1100]\tvalid_0's l1: 6.14598\n",
      "[1200]\tvalid_0's l1: 6.13652\n",
      "[1300]\tvalid_0's l1: 6.12754\n",
      "[1400]\tvalid_0's l1: 6.1183\n",
      "[1500]\tvalid_0's l1: 6.10968\n",
      "[1600]\tvalid_0's l1: 6.10052\n",
      "[1700]\tvalid_0's l1: 6.09149\n",
      "[1800]\tvalid_0's l1: 6.08358\n",
      "[1900]\tvalid_0's l1: 6.07546\n",
      "[2000]\tvalid_0's l1: 6.06726\n",
      "[2100]\tvalid_0's l1: 6.05955\n",
      "[2200]\tvalid_0's l1: 6.05176\n",
      "[2300]\tvalid_0's l1: 6.04375\n",
      "[2400]\tvalid_0's l1: 6.03631\n",
      "[2500]\tvalid_0's l1: 6.02908\n",
      "[2600]\tvalid_0's l1: 6.02087\n",
      "[2700]\tvalid_0's l1: 6.0137\n",
      "[2800]\tvalid_0's l1: 6.00681\n",
      "[2900]\tvalid_0's l1: 6.00011\n",
      "[3000]\tvalid_0's l1: 5.99352\n",
      "[3100]\tvalid_0's l1: 5.98607\n",
      "[3200]\tvalid_0's l1: 5.97933\n",
      "[3300]\tvalid_0's l1: 5.97245\n",
      "[3400]\tvalid_0's l1: 5.96631\n",
      "[3500]\tvalid_0's l1: 5.95977\n",
      "[3600]\tvalid_0's l1: 5.95369\n",
      "[3700]\tvalid_0's l1: 5.94713\n",
      "[3800]\tvalid_0's l1: 5.94153\n",
      "[3900]\tvalid_0's l1: 5.93535\n",
      "[4000]\tvalid_0's l1: 5.92977\n",
      "[4100]\tvalid_0's l1: 5.92339\n",
      "[4200]\tvalid_0's l1: 5.91793\n",
      "[4300]\tvalid_0's l1: 5.91239\n",
      "[4400]\tvalid_0's l1: 5.90709\n",
      "[4500]\tvalid_0's l1: 5.90066\n",
      "[4600]\tvalid_0's l1: 5.89525\n",
      "[4700]\tvalid_0's l1: 5.88973\n",
      "[4800]\tvalid_0's l1: 5.88409\n",
      "[4900]\tvalid_0's l1: 5.87839\n",
      "[5000]\tvalid_0's l1: 5.87226\n",
      "[5100]\tvalid_0's l1: 5.86656\n",
      "[5200]\tvalid_0's l1: 5.86136\n",
      "[5300]\tvalid_0's l1: 5.85621\n",
      "[5400]\tvalid_0's l1: 5.8508\n",
      "[5500]\tvalid_0's l1: 5.84563\n",
      "[5600]\tvalid_0's l1: 5.84107\n",
      "[5700]\tvalid_0's l1: 5.83621\n",
      "[5800]\tvalid_0's l1: 5.83093\n",
      "[5900]\tvalid_0's l1: 5.82592\n",
      "[6000]\tvalid_0's l1: 5.82131\n",
      "[6100]\tvalid_0's l1: 5.8163\n",
      "[6200]\tvalid_0's l1: 5.81111\n",
      "[6300]\tvalid_0's l1: 5.80596\n",
      "[6400]\tvalid_0's l1: 5.80061\n",
      "[6500]\tvalid_0's l1: 5.79586\n",
      "[6600]\tvalid_0's l1: 5.79136\n",
      "[6700]\tvalid_0's l1: 5.78697\n",
      "[6800]\tvalid_0's l1: 5.78228\n",
      "[6900]\tvalid_0's l1: 5.77713\n",
      "[7000]\tvalid_0's l1: 5.77263\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[7000]\tvalid_0's l1: 5.77263\n",
      "Model for fold 3 saved to modelitos_para_despues\\doblez_3.txt\n",
      ":LGB Fold 3 MAE: 5.772628092854403\n",
      "Fold 4 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.96775\n",
      "[200]\tvalid_0's l1: 5.92817\n",
      "[300]\tvalid_0's l1: 5.89756\n",
      "[400]\tvalid_0's l1: 5.87105\n",
      "[500]\tvalid_0's l1: 5.84865\n",
      "[600]\tvalid_0's l1: 5.82818\n",
      "[700]\tvalid_0's l1: 5.80892\n",
      "[800]\tvalid_0's l1: 5.79111\n",
      "[900]\tvalid_0's l1: 5.77477\n",
      "[1000]\tvalid_0's l1: 5.75965\n",
      "[1100]\tvalid_0's l1: 5.74562\n",
      "[1200]\tvalid_0's l1: 5.73224\n",
      "[1300]\tvalid_0's l1: 5.71961\n",
      "[1400]\tvalid_0's l1: 5.70753\n",
      "[1500]\tvalid_0's l1: 5.69636\n",
      "[1600]\tvalid_0's l1: 5.68532\n",
      "[1700]\tvalid_0's l1: 5.67515\n",
      "[1800]\tvalid_0's l1: 5.6648\n",
      "[1900]\tvalid_0's l1: 5.65451\n",
      "[2000]\tvalid_0's l1: 5.64487\n",
      "[2100]\tvalid_0's l1: 5.63484\n",
      "[2200]\tvalid_0's l1: 5.62523\n",
      "[2300]\tvalid_0's l1: 5.61596\n",
      "[2400]\tvalid_0's l1: 5.60657\n",
      "[2500]\tvalid_0's l1: 5.5975\n",
      "[2600]\tvalid_0's l1: 5.58839\n",
      "[2700]\tvalid_0's l1: 5.57941\n",
      "[2800]\tvalid_0's l1: 5.57048\n",
      "[2900]\tvalid_0's l1: 5.5613\n",
      "[3000]\tvalid_0's l1: 5.5529\n",
      "[3100]\tvalid_0's l1: 5.54421\n",
      "[3200]\tvalid_0's l1: 5.5364\n",
      "[3300]\tvalid_0's l1: 5.5278\n",
      "[3400]\tvalid_0's l1: 5.51967\n",
      "[3500]\tvalid_0's l1: 5.51156\n",
      "[3600]\tvalid_0's l1: 5.50373\n",
      "[3700]\tvalid_0's l1: 5.49604\n",
      "[3800]\tvalid_0's l1: 5.48865\n",
      "[3900]\tvalid_0's l1: 5.48107\n",
      "[4000]\tvalid_0's l1: 5.47343\n",
      "[4100]\tvalid_0's l1: 5.46657\n",
      "[4200]\tvalid_0's l1: 5.45958\n",
      "[4300]\tvalid_0's l1: 5.45257\n",
      "[4400]\tvalid_0's l1: 5.44542\n",
      "[4500]\tvalid_0's l1: 5.43859\n",
      "[4600]\tvalid_0's l1: 5.43187\n",
      "[4700]\tvalid_0's l1: 5.42551\n",
      "[4800]\tvalid_0's l1: 5.41907\n",
      "[4900]\tvalid_0's l1: 5.41248\n",
      "[5000]\tvalid_0's l1: 5.40592\n",
      "[5100]\tvalid_0's l1: 5.39913\n",
      "[5200]\tvalid_0's l1: 5.39247\n",
      "[5300]\tvalid_0's l1: 5.38595\n",
      "[5400]\tvalid_0's l1: 5.37947\n",
      "[5500]\tvalid_0's l1: 5.37349\n",
      "[5600]\tvalid_0's l1: 5.36759\n",
      "[5700]\tvalid_0's l1: 5.36142\n",
      "[5800]\tvalid_0's l1: 5.35504\n",
      "[5900]\tvalid_0's l1: 5.34942\n",
      "[6000]\tvalid_0's l1: 5.34384\n",
      "[6100]\tvalid_0's l1: 5.3385\n",
      "[6200]\tvalid_0's l1: 5.33296\n",
      "[6300]\tvalid_0's l1: 5.3271\n",
      "[6400]\tvalid_0's l1: 5.32204\n",
      "[6500]\tvalid_0's l1: 5.31667\n",
      "[6600]\tvalid_0's l1: 5.31099\n",
      "[6700]\tvalid_0's l1: 5.30552\n",
      "[6800]\tvalid_0's l1: 5.29965\n",
      "[6900]\tvalid_0's l1: 5.29434\n",
      "[7000]\tvalid_0's l1: 5.28864\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[7000]\tvalid_0's l1: 5.28864\n",
      "Model for fold 4 saved to modelitos_para_despues\\doblez_4.txt\n",
      ":LGB Fold 4 MAE: 5.288644790138467\n",
      "Fold 5 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 4.83894\n",
      "[200]\tvalid_0's l1: 4.82351\n",
      "[300]\tvalid_0's l1: 4.81772\n",
      "[400]\tvalid_0's l1: 4.814\n",
      "Early stopping, best iteration is:\n",
      "[390]\tvalid_0's l1: 4.81382\n",
      "Model for fold 5 saved to modelitos_para_despues\\doblez_5.txt\n",
      ":LGB Fold 5 MAE: 4.8138156559302905\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if LGB:\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    lgb_params = {\n",
    "        \"objective\": \"mae\",\n",
    "        \"n_estimators\": 7000,\n",
    "        \"num_leaves\": 256,\n",
    "        \"subsample\": 0.6,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "#         \"learning_rate\": 0.00871,\n",
    "        \"learning_rate\": 0.01,\n",
    "        'max_depth': 11,\n",
    "        \"n_jobs\": 4,\n",
    "        \"device\": \"gpu\",\n",
    "        \"gpu_platform_id\": 1,  # Set the OpenCL platform (default: 0)\n",
    "        \"gpu_device_id\": 0,  # Set the specific GPU (0 or 1)\n",
    "        \"num_threads\": -1,  \n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\": \"gain\",\n",
    "#         \"reg_alpha\": 0.1,\n",
    "        \"reg_alpha\": 0.2,\n",
    "        \"reg_lambda\": 3.25\n",
    "    }\n",
    "\n",
    "    feature_columns = list(df_train_feats.columns)\n",
    "    print(f\"Features = {len(feature_columns)}\")\n",
    "    #print(f\"Feature length = {len(feature_columns)}\")\n",
    "\n",
    "    num_folds = 5\n",
    "    fold_size = 480 // num_folds\n",
    "    gap = 5\n",
    "\n",
    "    models = []\n",
    "    models_cbt = []\n",
    "    scores = []\n",
    "\n",
    "    model_save_path = 'modelitos_para_despues' \n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    date_ids = df_train['date_id'].values\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        if i < num_folds - 1:  # No need to purge after the last fold\n",
    "            purged_start = end - 2\n",
    "            purged_end = end + gap + 2\n",
    "            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "        else:\n",
    "            train_indices = (date_ids >= start) & (date_ids < end)\n",
    "\n",
    "        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        df_fold_train = df_train_feats[train_indices]\n",
    "        df_fold_train_target = df_train['target'][train_indices]\n",
    "        df_fold_valid = df_train_feats[test_indices]\n",
    "        df_fold_valid_target = df_train['target'][test_indices]\n",
    "\n",
    "        print(f\"Fold {i+1} Model Training\")\n",
    "\n",
    "        # Train a LightGBM model for the current fold\n",
    "        lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_model.fit(\n",
    "            df_fold_train[feature_columns],\n",
    "            df_fold_train_target,\n",
    "            eval_set=[(df_fold_valid[feature_columns], df_fold_valid_target)],\n",
    "            callbacks=[\n",
    "                lgb.callback.early_stopping(stopping_rounds=100),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "\n",
    "        models.append(lgb_model)\n",
    "        # Save the model to a file\n",
    "        model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n",
    "        lgb_model.booster_.save_model(model_filename)\n",
    "        print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "        # Evaluate model performance on the validation set\n",
    "        #------------LGB--------------#\n",
    "        fold_predictions = lgb_model.predict(df_fold_valid[feature_columns])\n",
    "        fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "        scores.append(fold_score)\n",
    "        print(f\":LGB Fold {i+1} MAE: {fold_score}\")\n",
    "\n",
    "        # Free up memory by deleting fold specific variables\n",
    "        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "        gc.collect()\n",
    "\n",
    "    # Calculate the average best iteration from all regular folds\n",
    "    average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "    # Update the lgb_params with the average best iteration\n",
    "    final_model_params = lgb_params.copy()\n",
    "\n",
    "    # final_model_params['n_estimators'] = average_best_iteration\n",
    "    # print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "\n",
    "    # Train the final model on the entire dataset\n",
    "    num_model = 1\n",
    "\n",
    "    for i in range(num_model):\n",
    "        final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "        final_model.fit(\n",
    "            df_train_feats[feature_columns],\n",
    "            df_train['target'],\n",
    "            callbacks=[\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "        # Append the final model to the list of models\n",
    "        models.append(final_model)\n",
    "    model_filename = os.path.join(model_save_path, f'doblez.txt')\n",
    "    final_model.booster_.save_model(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc8c45",
   "metadata": {
    "papermill": {
     "duration": 0.041895,
     "end_time": "2023-12-17T08:50:03.949067",
     "exception": false,
     "start_time": "2023-12-17T08:50:03.907172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f550c4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T08:50:04.018685Z",
     "iopub.status.busy": "2023-12-17T08:50:04.018324Z",
     "iopub.status.idle": "2023-12-17T08:50:10.190599Z",
     "shell.execute_reply": "2023-12-17T08:50:10.189551Z"
    },
    "papermill": {
     "duration": 6.20787,
     "end_time": "2023-12-17T08:50:10.193032",
     "exception": false,
     "start_time": "2023-12-17T08:50:03.985162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optiver2023.competition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_infer:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptiver2023\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     env \u001b[38;5;241m=\u001b[39m optiver2023\u001b[38;5;241m.\u001b[39mmake_env()\n\u001b[0;32m     12\u001b[0m     iter_test \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39miter_test()\n",
      "File \u001b[1;32mc:\\Users\\wangn\\OneDrive - TNO\\Resources\\Kaggle\\Optiver代码+简历\\optiver2023\\__init__.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompetition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_env\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake_env\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optiver2023.competition'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def weighted_average(a):\n",
    "    w = []\n",
    "    n = len(a)\n",
    "    for j in range(1, n + 1):\n",
    "        j = 2 if j == 1 else j\n",
    "        w.append(1 / n)\n",
    "    return w\n",
    "\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "    cachey = pd.DataFrame()\n",
    "    stocklist = list(df.stock_id.unique())\n",
    "\n",
    "    lgb_model_weights = weighted_average(models)\n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        columns_given = ['seconds_in_bucket', 'imbalance_size',\n",
    "         'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
    "         'far_price', 'near_price', 'bid_price', 'bid_size',\n",
    "         'ask_price', 'ask_size', 'wap',]\n",
    "        test[columns_given] = test[columns_given].astype(float)\n",
    "        test['time_id'] = test['date_id'].astype(str) +'_'+test['seconds_in_bucket'].astype(str)\n",
    "\n",
    "        cachey = pd.concat([cachey, revealed_targets], ignore_index=True, axis=0)\n",
    "        cachey = cachey[cachey.stock_id.isin(stocklist)].sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id'])\n",
    "        cachey['revealed_target'] = cachey.revealed_target.astype(float)\n",
    "        cachey['target_shift1'] = cachey.groupby(['stock_id','seconds_in_bucket'])['revealed_target'].shift(0)\n",
    "        \n",
    "        test = pd.merge(test,cachey[['date_id', 'stock_id', 'seconds_in_bucket','target_shift1']],\\\n",
    "                         how='left',on = ['date_id', 'stock_id','seconds_in_bucket'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        \n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        if test.currently_scored.iloc[0]== False:\n",
    "            sample_prediction['target'] = 0\n",
    "            env.predict(sample_prediction)\n",
    "            counter += 1\n",
    "            qps.append(time.time() - now_time)\n",
    "            if counter % 10 == 0:\n",
    "                print(counter, 'qps:', np.mean(qps))\n",
    "            continue\n",
    "        try:\n",
    "            feat = generate_all_features(cache)[-len(test):]\n",
    "            print(f\"Feat Shape is: {feat.shape}\")\n",
    "\n",
    "            # Generate predictions for each model and calculate the weighted average\n",
    "            if LGB:\n",
    "                lgb_predictions = np.zeros(len(test))\n",
    "                for model, weight in zip(models, lgb_model_weights):\n",
    "                    lgb_predictions += weight * model.predict(feat[feature_columns])\n",
    "\n",
    "            predictions = lgb_predictions\n",
    "\n",
    "            #Using mean predictions rather than zero sum\n",
    "            final_predictions = predictions - np.mean(predictions)\n",
    "            clipped_predictions = np.clip(final_predictions, y_min, y_max)\n",
    "            sample_prediction['target'] = clipped_predictions\n",
    "        except:\n",
    "            sample_prediction['target'] = 0\n",
    "\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b291469",
   "metadata": {
    "papermill": {
     "duration": 0.034852,
     "end_time": "2023-12-17T08:50:10.263076",
     "exception": false,
     "start_time": "2023-12-17T08:50:10.228224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12896.511879,
   "end_time": "2023-12-17T08:50:13.169072",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-17T05:15:16.657193",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
